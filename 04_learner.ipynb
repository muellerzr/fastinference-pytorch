{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learner\n",
    "> The main interface for this library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# We only import .* so all transforms are in namespace\n",
    "from fastinference_pytorch.transforms.data import *\n",
    "from fastinference_pytorch.transforms.vision import *\n",
    "from fastinference_pytorch.utils import to_device, tensor, to_numpy\n",
    "from fastinference_pytorch.rebuild import make_pipelines, generate_pipeline, load_model, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "from fastcore.utils import store_attr\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `data_funcs`\n",
    "\n",
    "`data_funcs` are designed to be quick references to tell your `Learner` how your data is going to come in. Below is an example for grabbing images, however this should be however you expect your data to be coming in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_image(fn, mode='RGB'): return PILBase.create(fn,mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction functions\n",
    "\n",
    "These are prediction functions for a `PyTorch` model and an `ONNX` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def torch_preds(batch, model):\n",
    "    \"Get predictions from torch model\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(batch)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def onnx_preds(batch, model):\n",
    "    if isinstance(batch[0], Tensor): inps = [to_numpy(x) for x in batch]\n",
    "    names = [i.name for i in model.get_inputs()]\n",
    "    xs = {name:x for name,x in zip(names, batch)}\n",
    "    return tensor(model.run(None, xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDataset():\n",
    "    def __init__(self, data, pipelines):\n",
    "        \"Generic dataset with `pipelines` on `get_item`\"\n",
    "        self.pipelines = make_pipelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Learner():\n",
    "    \"\"\"\n",
    "    Similar to a `fastai` learner for inference\n",
    "\n",
    "    Params:\n",
    "      > `path` (str): The exact path to where your data and model is stored, relative to the `cwd`\n",
    "      > `data_fn` (str): Filename of your pickled data\n",
    "      > `model_fn` (str): Filename of your model\n",
    "      > `data_func` (function): A function in which has the ability to grab your data based on some input.\n",
    "                     The default grabs an image in a location and opens it with Pillow\n",
    "      > `bs` (int): The batch size you are wanting to use per inference (this can be tweaked later)\n",
    "      > `cpu` (bool): Whether to use the CPU or GPU\n",
    "      > `onnx` (bool): Whether the model is expected to be PyTorch or ONNX format\n",
    "\n",
    "    Example use:\n",
    "\n",
    "    learn = Learner('models/data', 'models/model', data_func=image_getter, bs=4, cpu=True)\n",
    "    \"\"\"\n",
    "    def __init__(self, path = Path('.'), data_fn='data', model_fn='model', data_func=None, bs=16, cpu=False, onnx=False):\n",
    "        data = load_data(path, data_fn)\n",
    "        self.n_inp = data['n_inp']\n",
    "        self.pipelines = make_pipelines(data)\n",
    "        self.after_item = self.pipelines['after_item']\n",
    "        self.after_batch = self.pipelines['after_batch']\n",
    "        self.tfm_y = generate_pipeline(data['tfms'], order=False)\n",
    "        self.model = load_model(path, model_fn, cpu, onnx)\n",
    "        self.device = 'cpu' if cpu else 'cuda'\n",
    "        store_attr(self, 'data_func,bs')\n",
    "        self.decode_func = None\n",
    "        \n",
    "    def _make_data(self, data):\n",
    "        \"Passes `data` through `after_item` and `after_batch`, splitting into batches\"\n",
    "        self.n_batches = len(data) // self.bs + (0 if len(data)%self.bs == 0 else 1)\n",
    "        batch,batches = [],[]\n",
    "        for d in data:\n",
    "            d = self.data_func(d)\n",
    "            for tfm in self.after_item: d = tfm(d)\n",
    "            batch.append(d)\n",
    "            if len(batch) == self.bs or (len(batches) == self.n_batches - 1 and len(batch) == len(data)):\n",
    "                batch = torch.stack(batch, dim=0)\n",
    "                batch = to_device(batch, self.device)\n",
    "                for tfm in self.after_batch: \n",
    "                    batch = tfm(batch)\n",
    "                batches.append(batch)\n",
    "                batch = []\n",
    "        return batches\n",
    "    \n",
    "    def _decode_inputs(self, inps, outs):\n",
    "        \"Decodes images through `after_batch`\"\n",
    "        for tfm in self.after_batch[::-1]:\n",
    "            if hasattr(tfm, 'can_decode'):\n",
    "                inps = to_device(tfm(inps, decode=True))\n",
    "        outs.insert(len(outs), inps)\n",
    "        return outs\n",
    "    \n",
    "    def get_preds(self, data, raw_outs=False, decode_func=None, with_input=False):\n",
    "        \"\"\"\n",
    "        Gather predictions on `data` with possible decoding. \n",
    "        \n",
    "        Params:\n",
    "          > `data`: Incoming data formatted to what `self.data_func`is expecting\n",
    "          > `raw_outs`: Whether to return the raw outputs\n",
    "          > `decode_func`: A function to use for decoding potential outputs.\n",
    "                           While the default is `None`, see `decode_cel` for an example\n",
    "          > `with_input`: Whether to return a decoded input up to what the model was passed\n",
    "        \"\"\"\n",
    "        inps, outs, dec_out, raw = [],[],[],[]\n",
    "        batches = self._make_data(data)\n",
    "        if self.n_inp > 1:\n",
    "            [inps.append([]) for _ in range(n_inp)]\n",
    "        for batch in batches:\n",
    "            if with_input:\n",
    "                if self.n_inp > 1:\n",
    "                    for i in range(self.n_inp):\n",
    "                        inps[i].append(batch[i].cpu())\n",
    "                    else:\n",
    "                        inps.append(batch[0].cpu())\n",
    "            if onnx: out = onnx_preds(batch[:self.n_inp], self.model)\n",
    "            else: out = torch_preds(batch[:self.n_inp], self.model)\n",
    "            raw.append(out)\n",
    "            if self.decode_func is not None: dec_out.append(decode_func(out))\n",
    "        raw = torch.cat(raw, dim=0).cpu().numpy()\n",
    "        if self.decode_func is not None: dec_out = torch.cat(dec_out, dim=0)\n",
    "        if self.decode_func is None or raw_outs: outs.insert(0, raw)\n",
    "        else: outs.insert(0, dec_out)\n",
    "        if with_input: outs = self._decode_inputs(inps, outs)\n",
    "        return outs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
