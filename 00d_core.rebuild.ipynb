{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core.rebuild"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core.rebuild\n",
    "> Functions needed to rebuild vision-based transforms from export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import io, operator, pickle, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "from numpy import ndarray\n",
    "from fastcore.utils import _patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastinference_pytorch.utils import to_device, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpickling export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_data(path:Path=Path('.'), fn='data'):\n",
    "    \"Opens `pkl` file containing exported `Transform` information\"\n",
    "    if '.pkl' not in fn: fn += '.pkl'\n",
    "    if not isinstance(path, Path): path = Path(path)\n",
    "    with open(path/fn, 'rb') as handle:\n",
    "        tfmd_dict = pickle.load(handle)\n",
    "    return tfmd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_model(path:Path='.', fn='model', cpu=True, onnx=False): \n",
    "    if not onnx:\n",
    "        if '.pkl' not in fn: fn += '.pkl'\n",
    "        path = f'{path}/{fn}'\n",
    "        return torch.load(path, map_location='cpu' if cpu else None)\n",
    "    else:\n",
    "        try: \n",
    "            import onnxruntime as ort\n",
    "        except ImportError:\n",
    "            print('to use ONNX you must have `onnxruntime` installed\\n\\\n",
    "            Install it with `pip install onnxruntime-gpu`')\n",
    "        if '.onnx' not in fn: fn += '.onnx'\n",
    "        path = f'{path}/{fn}'\n",
    "        ort_session = ort.InferenceSession(path)\n",
    "        if not cpu:\n",
    "            try:\n",
    "                ort_session.set_providers(['CUDAExecutionProvider'])\n",
    "            except:\n",
    "                ort_session.set_providers(['CPUExecutionProvider'])\n",
    "        return ort_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "These generate our transform \"pipelines\" (as we're not using `fastcore`'s `Pipeline`) to pass our data through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_tfm(key, tfms): \n",
    "    \"Makes a transform from `key`. Class or function must be in global memory (imported)\"\n",
    "    args = tfms[key]\n",
    "    return globals()[key](**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate_pipeline(tfms, order=True) -> dict:\n",
    "    \"Generate `pipe` of transforms from dict and (potentially) sort them\"\n",
    "    pipe = []\n",
    "    for key in tfms.keys():\n",
    "        tfm = get_tfm(key, tfms)\n",
    "        pipe.append(tfm)\n",
    "    if order: pipe = sorted(pipe, key=operator.attrgetter('order'))\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_pipelines(tfms) -> dict: \n",
    "    \"Make `item` and `batch` transform pipelines\"\n",
    "    pipe, keys = {}, ['after_item', 'after_batch']\n",
    "    for key in keys:\n",
    "        pipe[key] = generate_pipeline(tfms[key], True)\n",
    "    if not any(isinstance(x, ToTensor) for x in pipe['after_item']):\n",
    "        pipe['after_item'].append(ToTensor(False))\n",
    "    return pipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
